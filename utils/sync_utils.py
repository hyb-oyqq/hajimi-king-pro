import json
import threading
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional

import requests

from common.Logger import logger
from common.config import Config
from common.translations import get_translator
from common import state
from utils.file_manager import file_manager, checkpoint

# è·å–ç¿»è¯‘å‡½æ•°
t = get_translator().t


class SyncUtils:
    """åŒæ­¥å·¥å…·ç±»ï¼Œè´Ÿè´£å¼‚æ­¥å‘é€keysåˆ°å¤–éƒ¨åº”ç”¨"""

    def __init__(self):
        """åˆå§‹åŒ–åŒæ­¥å·¥å…·"""
        # Gemini Balancer é…ç½®
        self.balancer_url = Config.GEMINI_BALANCER_URL.rstrip('/') if Config.GEMINI_BALANCER_URL else ""
        self.balancer_auth = Config.GEMINI_BALANCER_AUTH
        self.balancer_sync_enabled = Config.parse_bool(Config.GEMINI_BALANCER_SYNC_ENABLED)
        self.balancer_enabled = bool(self.balancer_url and self.balancer_auth and self.balancer_sync_enabled)

        # GPT-load é…ç½®
        self.gpt_load_url = Config.GPT_LOAD_URL.rstrip('/') if Config.GPT_LOAD_URL else ""
        self.gpt_load_auth = Config.GPT_LOAD_AUTH
        # è§£æå¤šä¸ªgroup names (é€—å·åˆ†éš”)
        self.gpt_load_group_names = [name.strip() for name in Config.GPT_LOAD_GROUP_NAME.split(',') if name.strip()] if Config.GPT_LOAD_GROUP_NAME else []
        self.gpt_load_sync_enabled = Config.parse_bool(Config.GPT_LOAD_SYNC_ENABLED)
        self.gpt_load_enabled = bool(self.gpt_load_url and self.gpt_load_auth and self.gpt_load_group_names and self.gpt_load_sync_enabled)

        # GPT-load - Paid Keys é…ç½®
        self.gpt_load_paid_group_name = Config.GPT_LOAD_PAID_GROUP_NAME.strip() if Config.GPT_LOAD_PAID_GROUP_NAME else ""
        self.gpt_load_paid_sync_enabled = Config.parse_bool(Config.GPT_LOAD_PAID_SYNC_ENABLED)
        self.gpt_load_paid_enabled = bool(self.gpt_load_url and self.gpt_load_auth and self.gpt_load_paid_group_name and self.gpt_load_paid_sync_enabled)

        # GPT-load - Rate Limited Keys é…ç½®
        self.gpt_load_rate_limited_group_name = Config.GPT_LOAD_RATE_LIMITED_GROUP_NAME.strip() if Config.GPT_LOAD_RATE_LIMITED_GROUP_NAME else ""
        self.rate_limited_handling = Config.RATE_LIMITED_HANDLING.strip().lower()
        self.gpt_load_rate_limited_enabled = bool(
            self.gpt_load_url and 
            self.gpt_load_auth and 
            self.gpt_load_rate_limited_group_name and 
            self.rate_limited_handling == "sync_separate"
        )

        # åˆ›å»ºçº¿ç¨‹æ± ç”¨äºå¼‚æ­¥æ‰§è¡Œ
        self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="SyncUtils")
        self.saving_checkpoint = False

        # å‘¨æœŸæ€§å‘é€æ§åˆ¶
        self.batch_interval = 60
        self.batch_timer = None
        self.shutdown_flag = False

        # GPT-load group ID ç¼“å­˜ (15åˆ†é’Ÿç¼“å­˜)
        self.group_id_cache: Dict[str, int] = {}
        self.group_id_cache_time: Dict[str, float] = {}
        self.group_id_cache_ttl = 15 * 60  # 15åˆ†é’Ÿ

        if not self.balancer_enabled:
            logger.warning(t('balancer_sync_disabled'))
        else:
            logger.info(t('balancer_enabled_url', self.balancer_url))

        if not self.gpt_load_enabled:
            logger.warning(t('gpt_load_sync_disabled'))
        else:
            logger.info(t('gpt_load_enabled_url', self.gpt_load_url, ', '.join(self.gpt_load_group_names)))

        if not self.gpt_load_paid_enabled:
            logger.warning("ğŸ’ ä»˜è´¹å¯†é’¥ä¸Šä¼ åŠŸèƒ½æœªå¯ç”¨")
        else:
            logger.info(f"ğŸ’ ä»˜è´¹å¯†é’¥ä¸Šä¼ å·²å¯ç”¨: {self.gpt_load_url} -> åˆ†ç»„: {self.gpt_load_paid_group_name}")

        # 429å¯†é’¥å¤„ç†ç­–ç•¥æ—¥å¿—
        logger.info(f"â° 429å¯†é’¥å¤„ç†ç­–ç•¥: {self.rate_limited_handling}")
        if self.gpt_load_rate_limited_enabled:
            logger.info(f"â° 429å¯†é’¥å•ç‹¬åˆ†ç»„å·²å¯ç”¨: {self.gpt_load_url} -> åˆ†ç»„: {self.gpt_load_rate_limited_group_name}")

        # å¯åŠ¨å‘¨æœŸæ€§å‘é€çº¿ç¨‹
        self._start_batch_sender()

    def add_keys_to_queue(self, keys: List[str]):
        """
        å°†keysåŒæ—¶æ·»åŠ åˆ°balancerå’ŒGPT loadçš„å‘é€é˜Ÿåˆ—
        
        Args:
            keys: API keysåˆ—è¡¨
        """
        if not keys:
            return

        # Acquire lock for checkpoint saving
        while self.saving_checkpoint:
            logger.info(t('checkpoint_saving_wait', len(keys)))
            time.sleep(0.5)  # Small delay to prevent busy-waiting

        self.saving_checkpoint = True  # Acquire the lock
        try:

            # Gemini Balancer
            if self.balancer_enabled:
                initial_balancer_count = len(checkpoint.wait_send_balancer)
                checkpoint.wait_send_balancer.update(keys)
                new_balancer_count = len(checkpoint.wait_send_balancer)
                added_balancer_count = new_balancer_count - initial_balancer_count
                logger.info(t('added_to_balancer_queue', added_balancer_count, new_balancer_count))
            else:
                logger.info(t('balancer_disabled_skipping', len(keys)))

            # GPT-load
            if self.gpt_load_enabled:
                initial_gpt_count = len(checkpoint.wait_send_gpt_load)
                checkpoint.wait_send_gpt_load.update(keys)
                new_gpt_count = len(checkpoint.wait_send_gpt_load)
                added_gpt_count = new_gpt_count - initial_gpt_count
                logger.info(t('added_to_gpt_load_queue', added_gpt_count, new_gpt_count))
            else:
                logger.info(t('gpt_load_disabled_skipping', len(keys)))

            file_manager.save_checkpoint(checkpoint)
        finally:
            self.saving_checkpoint = False  # Release the lock

    def add_paid_keys_to_queue(self, keys: List[str]):
        """
        å°†ä»˜è´¹å¯†é’¥æ·»åŠ åˆ°GPT-loadçš„ç‹¬ç«‹åˆ†ç»„é˜Ÿåˆ—
        
        Args:
            keys: ä»˜è´¹API keysåˆ—è¡¨
        """
        if not keys:
            return

        # Acquire lock for checkpoint saving
        while self.saving_checkpoint:
            logger.info(f"ğŸ’ ç­‰å¾…checkpointä¿å­˜å®Œæˆ... (ä»˜è´¹å¯†é’¥: {len(keys)} ä¸ª)")
            time.sleep(0.5)

        self.saving_checkpoint = True  # Acquire the lock
        try:
            # GPT-load - Paid Keys
            if self.gpt_load_paid_enabled:
                initial_paid_count = len(checkpoint.wait_send_gpt_load_paid)
                checkpoint.wait_send_gpt_load_paid.update(keys)
                new_paid_count = len(checkpoint.wait_send_gpt_load_paid)
                added_paid_count = new_paid_count - initial_paid_count
                logger.info(f"ğŸ’ å·²æ·»åŠ  {added_paid_count} ä¸ªä»˜è´¹å¯†é’¥åˆ°é˜Ÿåˆ— (æ€»è®¡: {new_paid_count})")
            else:
                logger.info(f"ğŸ’ ä»˜è´¹å¯†é’¥ä¸Šä¼ åŠŸèƒ½å·²å…³é—­ï¼Œè·³è¿‡ {len(keys)} ä¸ªä»˜è´¹å¯†é’¥")

            file_manager.save_checkpoint(checkpoint)
        finally:
            self.saving_checkpoint = False  # Release the lock

    def add_rate_limited_keys_to_queue(self, keys: List[str]):
        """
        å°†429é™é€Ÿå¯†é’¥æ·»åŠ åˆ°GPT-loadçš„ç‹¬ç«‹åˆ†ç»„é˜Ÿåˆ—
        
        Args:
            keys: 429é™é€ŸAPI keysåˆ—è¡¨
        """
        if not keys:
            return

        # Acquire lock for checkpoint saving
        while self.saving_checkpoint:
            logger.info(f"â° ç­‰å¾…checkpointä¿å­˜å®Œæˆ... (429å¯†é’¥: {len(keys)} ä¸ª)")
            time.sleep(0.5)

        self.saving_checkpoint = True  # Acquire the lock
        try:
            # GPT-load - Rate Limited Keys
            if self.gpt_load_rate_limited_enabled:
                initial_count = len(checkpoint.wait_send_gpt_load_rate_limited)
                checkpoint.wait_send_gpt_load_rate_limited.update(keys)
                new_count = len(checkpoint.wait_send_gpt_load_rate_limited)
                added_count = new_count - initial_count
                logger.info(f"â° å·²æ·»åŠ  {added_count} ä¸ª429å¯†é’¥åˆ°ç‹¬ç«‹é˜Ÿåˆ— (æ€»è®¡: {new_count})")
            else:
                logger.info(f"â° 429å¯†é’¥å•ç‹¬ä¸Šä¼ åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡ {len(keys)} ä¸ªå¯†é’¥")

            file_manager.save_checkpoint(checkpoint)
        finally:
            self.saving_checkpoint = False  # Release the lock

    def _send_balancer_worker(self, keys: List[str]) -> str:
        """
        å®é™…æ‰§è¡Œå‘é€åˆ°balancerçš„å·¥ä½œå‡½æ•°ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
        
        Args:
            keys: API keysåˆ—è¡¨
            
        Returns:
            str: "ok" if success, otherwise an error code string.
        """
        try:
            logger.info(t('sending_keys_to_balancer', len(keys)))

            # 1. è·å–å½“å‰é…ç½®
            config_url = f"{self.balancer_url}/api/config"
            headers = {
                'Cookie': f'auth_token={self.balancer_auth}',
                'User-Agent': 'HajimiKing/1.0'
            }

            logger.info(t('fetching_config_from', config_url))

            # è·å–å½“å‰é…ç½®
            response = requests.get(config_url, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(t('get_config_failed', response.status_code, response.text))
                return "get_config_failed_not_200"

            # è§£æé…ç½®
            config_data = response.json()

            # 2. è·å–å½“å‰çš„API_KEYSæ•°ç»„
            current_api_keys = config_data.get('API_KEYS', [])

            # 3. åˆå¹¶æ–°keysï¼ˆå»é‡ï¼‰
            existing_keys_set = set(current_api_keys)
            new_add_keys_set = set()
            for key in keys:
                if key not in existing_keys_set:
                    existing_keys_set.add(key)
                    new_add_keys_set.add(key)

            if len(new_add_keys_set) == 0:
                logger.info(t('all_keys_exist', len(keys)))
                # ä¸éœ€è¦è®°å½•å‘é€ç»“æœï¼Œå› ä¸ºæ²¡æœ‰å®é™…å‘é€æ–°å¯†é’¥
                return "ok"

            # 4. æ›´æ–°é…ç½®ä¸­çš„API_KEYS
            config_data['API_KEYS'] = list(existing_keys_set)

            logger.info(t('updating_balancer_config', len(new_add_keys_set)))

            # 5. å‘é€æ›´æ–°åçš„é…ç½®åˆ°æœåŠ¡å™¨
            update_headers = headers.copy()
            update_headers['Content-Type'] = 'application/json'

            update_response = requests.put(
                config_url,
                headers=update_headers,
                json=config_data,
                timeout=60
            )

            if update_response.status_code != 200:
                logger.error(t('update_config_failed', update_response.status_code, update_response.text))
                return "update_config_failed_not_200"

            # 6. éªŒè¯æ˜¯å¦æ·»åŠ æˆåŠŸ
            updated_config = update_response.json()
            updated_api_keys = updated_config.get('API_KEYS', [])
            updated_keys_set = set(updated_api_keys)

            failed_to_add = [key for key in new_add_keys_set if key not in updated_keys_set]

            if failed_to_add:
                logger.error(t('failed_to_add_keys', len(failed_to_add), [key[:10] + '...' for key in failed_to_add]))
                # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - éƒ¨åˆ†æˆåŠŸçš„æƒ…å†µ
                send_result = {}
                keys_to_log = []
                for key in new_add_keys_set:  # åªè®°å½•å°è¯•æ–°å¢çš„å¯†é’¥
                    if key in failed_to_add:
                        send_result[key] = "update_failed"
                        keys_to_log.append(key)
                    else:
                        send_result[key] = "ok"
                        keys_to_log.append(key)
                if keys_to_log:  # åªæœ‰å½“æœ‰éœ€è¦è®°å½•çš„å¯†é’¥æ—¶æ‰è®°å½•
                    file_manager.save_keys_send_result(keys_to_log, send_result)
                return "update_failed"


            logger.info(t('all_keys_added_successfully', len(new_add_keys_set)))
            
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - åªè®°å½•å®é™…æ–°å¢çš„å¯†é’¥
            send_result = {key: "ok" for key in new_add_keys_set}
            if send_result:  # åªæœ‰å½“æœ‰æ–°å¢å¯†é’¥æ—¶æ‰è®°å½•
                file_manager.save_keys_send_result(list(new_add_keys_set), send_result)
            
            return "ok"

        except requests.exceptions.Timeout:
            logger.error(t('request_timeout_balancer'))
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥
            send_result = {key: "timeout" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "timeout"
        except requests.exceptions.ConnectionError:
            logger.error(t('connection_failed_balancer'))
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥
            send_result = {key: "connection_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "connection_error"
        except json.JSONDecodeError as e:
            logger.error(t('invalid_json_balancer', str(e)))
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥
            send_result = {key: "json_decode_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "json_decode_error"
        except Exception as e:
            logger.error(t('failed_send_to_balancer', str(e)))
            traceback.print_exc()
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥
            send_result = {key: "exception" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "exception"

    def _get_gpt_load_group_id(self, group_name: str) -> Optional[int]:
        """
        è·å–GPT-load group IDï¼Œå¸¦ç¼“å­˜åŠŸèƒ½
        
        Args:
            group_name: ç»„å
            
        Returns:
            Optional[int]: ç»„IDï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if (group_name in self.group_id_cache and
            group_name in self.group_id_cache_time and
            current_time - self.group_id_cache_time[group_name] < self.group_id_cache_ttl):
            logger.info(t('using_cached_group_id', group_name, self.group_id_cache[group_name]))
            return self.group_id_cache[group_name]
        
        # ç¼“å­˜è¿‡æœŸæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è·å–
        try:
            groups_url = f"{self.gpt_load_url}/api/groups"
            headers = {
                'Authorization': f'Bearer {self.gpt_load_auth}',
                'User-Agent': 'HajimiKing/1.0'
            }

            logger.info(t('fetching_groups_from', groups_url))

            response = requests.get(groups_url, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(t('get_groups_failed', response.status_code, response.text))
                return None

            groups_data = response.json()
            
            if groups_data.get('code') != 0:
                logger.error(t('groups_api_error', groups_data.get('message', 'Unknown error')))
                return None

            # æŸ¥æ‰¾æŒ‡å®šgroupçš„ID
            groups_list = groups_data.get('data', [])
            for group in groups_list:
                if group.get('name') == group_name:
                    group_id = group.get('id')
                    # æ›´æ–°ç¼“å­˜
                    self.group_id_cache[group_name] = group_id
                    self.group_id_cache_time[group_name] = current_time
                    logger.info(t('found_and_cached_group', group_name, group_id))
                    return group_id

            logger.error(t('group_not_found', group_name))
            return None

        except Exception as e:
            logger.error(t('failed_get_group_id', group_name, str(e)))
            return None

    def _send_gpt_load_paid_worker(self, keys: List[str]) -> str:
        """
        å®é™…æ‰§è¡Œå‘é€ä»˜è´¹å¯†é’¥åˆ°GPT-loadçš„å·¥ä½œå‡½æ•°ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
        
        Args:
            keys: ä»˜è´¹API keysåˆ—è¡¨
            
        Returns:
            str: "ok" if success, otherwise an error code string.
        """
        try:
            logger.info(f"ğŸ’ æ­£åœ¨å‘é€ {len(keys)} ä¸ªä»˜è´¹å¯†é’¥åˆ°GPT-loadåˆ†ç»„: {self.gpt_load_paid_group_name}")

            # 1. è·å–group ID (ä½¿ç”¨ç¼“å­˜)
            group_id = self._get_gpt_load_group_id(self.gpt_load_paid_group_name)
            
            if group_id is None:
                logger.error(f"ğŸ’ è·å–ä»˜è´¹åˆ†ç»„IDå¤±è´¥: {self.gpt_load_paid_group_name}")
                return "failed_get_group_id"

            # 2. å‘é€keysåˆ°æŒ‡å®šgroup
            add_keys_url = f"{self.gpt_load_url}/api/keys/add-async"
            keys_text = ",".join(keys)
            
            add_headers = {
                'Authorization': f'Bearer {self.gpt_load_auth}',
                'Content-Type': 'application/json',
                'User-Agent': 'HajimiKing/1.0'
            }

            payload = {
                "group_id": group_id,
                "keys_text": keys_text
            }

            logger.info(f"ğŸ’ æ·»åŠ  {len(keys)} ä¸ªä»˜è´¹å¯†é’¥åˆ°åˆ†ç»„ [{self.gpt_load_paid_group_name}] (ID: {group_id})")

            # å‘é€æ·»åŠ keysè¯·æ±‚
            add_response = requests.post(
                add_keys_url,
                headers=add_headers,
                json=payload,
                timeout=60
            )

            if add_response.status_code != 200:
                logger.error(f"ğŸ’ æ·»åŠ ä»˜è´¹å¯†é’¥å¤±è´¥: HTTP {add_response.status_code} - {add_response.text}")
                return "add_keys_failed"

            # è§£ææ·»åŠ keyså“åº”
            add_data = add_response.json()
            
            if add_data.get('code') != 0:
                logger.error(f"ğŸ’ æ·»åŠ ä»˜è´¹å¯†é’¥APIé”™è¯¯: {add_data.get('message', 'Unknown error')}")
                return "add_keys_api_error"

            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            task_data = add_data.get('data', {})
            task_type = task_data.get('task_type')
            is_running = task_data.get('is_running')
            total = task_data.get('total', 0)

            logger.info(f"ğŸ’ ä»˜è´¹å¯†é’¥ä»»åŠ¡å·²å¯åŠ¨ [{self.gpt_load_paid_group_name}]")
            logger.info(f"ğŸ’ ä»»åŠ¡ç±»å‹: {task_type}, è¿è¡Œä¸­: {is_running}, æ€»æ•°: {total}")
            
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿—
            send_result = {key: "ok" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            
            return "ok"

        except requests.exceptions.Timeout:
            logger.error("ğŸ’ è¯·æ±‚è¶…æ—¶ - GPT-load (ä»˜è´¹å¯†é’¥)")
            send_result = {key: "timeout" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "timeout"
        except requests.exceptions.ConnectionError:
            logger.error("ğŸ’ è¿æ¥å¤±è´¥ - GPT-load (ä»˜è´¹å¯†é’¥)")
            send_result = {key: "connection_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "connection_error"
        except json.JSONDecodeError as e:
            logger.error(f"ğŸ’ JSONè§£æé”™è¯¯ - GPT-load (ä»˜è´¹å¯†é’¥): {str(e)}")
            send_result = {key: "json_decode_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "json_decode_error"
        except Exception as e:
            logger.error(f"ğŸ’ å‘é€ä»˜è´¹å¯†é’¥å¤±è´¥: {str(e)}", exc_info=True)
            send_result = {key: "exception" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "exception"

    def _send_gpt_load_rate_limited_worker(self, keys: List[str]) -> str:
        """
        å®é™…æ‰§è¡Œå‘é€429é™é€Ÿå¯†é’¥åˆ°GPT-loadçš„å·¥ä½œå‡½æ•°ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
        
        Args:
            keys: 429é™é€ŸAPI keysåˆ—è¡¨
            
        Returns:
            str: "ok" if success, otherwise an error code string.
        """
        try:
            logger.info(f"â° æ­£åœ¨å‘é€ {len(keys)} ä¸ª429å¯†é’¥åˆ°GPT-loadåˆ†ç»„: {self.gpt_load_rate_limited_group_name}")

            # 1. è·å–group ID (ä½¿ç”¨ç¼“å­˜)
            group_id = self._get_gpt_load_group_id(self.gpt_load_rate_limited_group_name)
            
            if group_id is None:
                logger.error(f"â° è·å–429åˆ†ç»„IDå¤±è´¥: {self.gpt_load_rate_limited_group_name}")
                return "failed_get_group_id"

            # 2. å‘é€keysåˆ°æŒ‡å®šgroup
            add_keys_url = f"{self.gpt_load_url}/api/keys/add-async"
            keys_text = ",".join(keys)
            
            add_headers = {
                'Authorization': f'Bearer {self.gpt_load_auth}',
                'Content-Type': 'application/json',
                'User-Agent': 'HajimiKing/1.0'
            }

            payload = {
                "group_id": group_id,
                "keys_text": keys_text
            }

            logger.info(f"â° æ·»åŠ  {len(keys)} ä¸ª429å¯†é’¥åˆ°åˆ†ç»„ [{self.gpt_load_rate_limited_group_name}] (ID: {group_id})")

            # å‘é€æ·»åŠ keysè¯·æ±‚
            add_response = requests.post(
                add_keys_url,
                headers=add_headers,
                json=payload,
                timeout=60
            )

            if add_response.status_code != 200:
                logger.error(f"â° æ·»åŠ 429å¯†é’¥å¤±è´¥: HTTP {add_response.status_code} - {add_response.text}")
                return "add_keys_failed"

            # è§£ææ·»åŠ keyså“åº”
            add_data = add_response.json()
            
            if add_data.get('code') != 0:
                logger.error(f"â° æ·»åŠ 429å¯†é’¥APIé”™è¯¯: {add_data.get('message', 'Unknown error')}")
                return "add_keys_api_error"

            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            task_data = add_data.get('data', {})
            task_type = task_data.get('task_type')
            is_running = task_data.get('is_running')
            total = task_data.get('total', 0)

            logger.info(f"â° 429å¯†é’¥ä»»åŠ¡å·²å¯åŠ¨ [{self.gpt_load_rate_limited_group_name}]")
            logger.info(f"â° ä»»åŠ¡ç±»å‹: {task_type}, è¿è¡Œä¸­: {is_running}, æ€»æ•°: {total}")
            
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿—
            send_result = {key: "ok_rate_limited" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            
            return "ok"

        except requests.exceptions.Timeout:
            logger.error("â° è¯·æ±‚è¶…æ—¶ - GPT-load (429å¯†é’¥)")
            send_result = {key: "timeout" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "timeout"
        except requests.exceptions.ConnectionError:
            logger.error("â° è¿æ¥å¤±è´¥ - GPT-load (429å¯†é’¥)")
            send_result = {key: "connection_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "connection_error"
        except json.JSONDecodeError as e:
            logger.error(f"â° JSONè§£æé”™è¯¯ - GPT-load (429å¯†é’¥): {str(e)}")
            send_result = {key: "json_decode_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "json_decode_error"
        except Exception as e:
            logger.error(f"â° å‘é€429å¯†é’¥å¤±è´¥: {str(e)}", exc_info=True)
            send_result = {key: "exception" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "exception"

    def _send_gpt_load_worker(self, keys: List[str]) -> str:
        """
        å®é™…æ‰§è¡Œå‘é€åˆ°GPT-loadçš„å·¥ä½œå‡½æ•°ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
        
        Args:
            keys: API keysåˆ—è¡¨
            
        Returns:
            str: "ok" if success, otherwise an error code string.
        """
        try:
            logger.info(t('sending_to_gpt_load', len(keys), len(self.gpt_load_group_names)))

            # éå†æ‰€æœ‰group namesï¼Œä¸ºæ¯ä¸ªgroupå‘é€keys
            all_success = True
            failed_groups = []
            
            for group_name in self.gpt_load_group_names:
                logger.info(t('processing_group', group_name))
                
                # 1. è·å–group ID (ä½¿ç”¨ç¼“å­˜)
                group_id = self._get_gpt_load_group_id(group_name)
                
                if group_id is None:
                    logger.error(t('failed_get_group_id_short', group_name))
                    failed_groups.append(group_name)
                    all_success = False
                    continue

                # 2. å‘é€keysåˆ°æŒ‡å®šgroup
                add_keys_url = f"{self.gpt_load_url}/api/keys/add-async"
                keys_text = ",".join(keys)
                
                add_headers = {
                    'Authorization': f'Bearer {self.gpt_load_auth}',
                    'Content-Type': 'application/json',
                    'User-Agent': 'HajimiKing/1.0'
                }

                payload = {
                    "group_id": group_id,
                    "keys_text": keys_text
                }

                logger.info(t('adding_keys_to_group', len(keys), group_name, group_id))

                try:
                    # å‘é€æ·»åŠ keysè¯·æ±‚
                    add_response = requests.post(
                        add_keys_url,
                        headers=add_headers,
                        json=payload,
                        timeout=60
                    )

                    if add_response.status_code != 200:
                        logger.error(t('failed_add_keys_to_group', group_name, add_response.status_code, add_response.text))
                        failed_groups.append(group_name)
                        all_success = False
                        continue

                    # è§£ææ·»åŠ keyså“åº”
                    add_data = add_response.json()
                    
                    if add_data.get('code') != 0:
                        logger.error(t('add_keys_api_error', group_name, add_data.get('message', 'Unknown error')))
                        failed_groups.append(group_name)
                        all_success = False
                        continue

                    # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                    task_data = add_data.get('data', {})
                    task_type = task_data.get('task_type')
                    is_running = task_data.get('is_running')
                    total = task_data.get('total', 0)
                    response_group_name = task_data.get('group_name')

                    logger.info(t('keys_task_started', group_name))
                    logger.info(t('task_type', task_type))
                    logger.info(t('is_running', is_running))
                    logger.info(t('total_keys', total))
                    logger.info(t('group_name', response_group_name))

                except Exception as e:
                    logger.error(t('exception_adding_to_group', group_name, str(e)))
                    failed_groups.append(group_name)
                    all_success = False
                    continue

            # æ ¹æ®ç»“æœè¿”å›çŠ¶æ€
            if all_success:
                logger.info(t('sent_to_all_groups', len(self.gpt_load_group_names)))
                # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½æˆåŠŸ
                send_result = {key: "ok" for key in keys}
                file_manager.save_keys_send_result(keys, send_result)
                return "ok"
            else:
                logger.error(t('failed_send_to_groups', len(failed_groups), ', '.join(failed_groups)))
                # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - éƒ¨åˆ†æˆ–å…¨éƒ¨å¤±è´¥
                send_result = {key: f"partial_failure_{len(failed_groups)}_groups" for key in keys}
                file_manager.save_keys_send_result(keys, send_result)
                return "partial_failure"

        except requests.exceptions.Timeout:
            logger.error(t('request_timeout_gpt_load'))
            send_result = {key: "timeout" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "timeout"
        except requests.exceptions.ConnectionError:
            logger.error(t('connection_failed_gpt_load'))
            send_result = {key: "connection_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "connection_error"
        except json.JSONDecodeError as e:
            logger.error(t('invalid_json_gpt_load', str(e)))
            send_result = {key: "json_decode_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "json_decode_error"
        except Exception as e:
            logger.error(t('failed_send_to_gpt_load', str(e)), exc_info=True)
            send_result = {key: "exception" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "exception"

    def _start_batch_sender(self) -> None:
        """å¯åŠ¨æ‰¹é‡å‘é€å®šæ—¶å™¨"""
        if self.shutdown_flag:
            return

        # å¯åŠ¨å‘é€ä»»åŠ¡
        self.executor.submit(self._batch_send_worker)

        # è®¾ç½®ä¸‹ä¸€æ¬¡å‘é€å®šæ—¶å™¨
        self.batch_timer = threading.Timer(self.batch_interval, self._start_batch_sender)
        self.batch_timer.daemon = True
        self.batch_timer.start()

    def _batch_send_worker(self) -> None:
        """æ‰¹é‡å‘é€worker"""
        # æ£€æŸ¥æ˜¯å¦å¤„äºå†·å´çŠ¶æ€
        if state.is_in_cooldown:
            logger.info("â„ï¸ ä¸»çº¿ç¨‹æ­£åœ¨å†·å´ä¸­ï¼Œè·³è¿‡æœ¬æ¬¡æ‰¹é‡å‘é€")
            return
        
        while self.saving_checkpoint:
            logger.info(t('checkpoint_saving_batch_wait'))
            time.sleep(0.5)

        self.saving_checkpoint = True
        try:
            # åŠ è½½checkpoint
            logger.info(t('starting_batch_send', len(checkpoint.wait_send_balancer), len(checkpoint.wait_send_gpt_load)))
            
            # å‘é€ä»˜è´¹å¯†é’¥é˜Ÿåˆ—
            if checkpoint.wait_send_gpt_load_paid and self.gpt_load_paid_enabled:
                paid_keys = list(checkpoint.wait_send_gpt_load_paid)
                logger.info(f"ğŸ’ å¤„ç†ä»˜è´¹å¯†é’¥é˜Ÿåˆ—: {len(paid_keys)} ä¸ª")

                result_code = self._send_gpt_load_paid_worker(paid_keys)

                if result_code == 'ok':
                    # æ¸…ç©ºé˜Ÿåˆ—
                    checkpoint.wait_send_gpt_load_paid.clear()
                    logger.info(f"ğŸ’ ä»˜è´¹å¯†é’¥é˜Ÿåˆ—å·²æ¸…ç©º: {len(paid_keys)} ä¸ªå¯†é’¥å·²å‘é€")
                else:
                    logger.error(f"ğŸ’ ä»˜è´¹å¯†é’¥é˜Ÿåˆ—å¤„ç†å¤±è´¥: {result_code}")
            
            # å‘é€429å¯†é’¥é˜Ÿåˆ—
            if checkpoint.wait_send_gpt_load_rate_limited and self.gpt_load_rate_limited_enabled:
                rate_limited_keys = list(checkpoint.wait_send_gpt_load_rate_limited)
                logger.info(f"â° å¤„ç†429å¯†é’¥é˜Ÿåˆ—: {len(rate_limited_keys)} ä¸ª")

                result_code = self._send_gpt_load_rate_limited_worker(rate_limited_keys)

                if result_code == 'ok':
                    # æ¸…ç©ºé˜Ÿåˆ—
                    checkpoint.wait_send_gpt_load_rate_limited.clear()
                    logger.info(f"â° 429å¯†é’¥é˜Ÿåˆ—å·²æ¸…ç©º: {len(rate_limited_keys)} ä¸ªå¯†é’¥å·²å‘é€")
                else:
                    logger.error(f"â° 429å¯†é’¥é˜Ÿåˆ—å¤„ç†å¤±è´¥: {result_code}")
            
            # å‘é€gemini balanceré˜Ÿåˆ—
            if checkpoint.wait_send_balancer and self.balancer_enabled:
                balancer_keys = list(checkpoint.wait_send_balancer)
                logger.info(t('processing_balancer_queue', len(balancer_keys)))

                result_code = self._send_balancer_worker(balancer_keys)
                if result_code == 'ok':
                    # æ¸…ç©ºé˜Ÿåˆ—
                    checkpoint.wait_send_balancer.clear()
                    logger.info(t('balancer_queue_cleared', len(balancer_keys)))
                else:
                    logger.error(t('balancer_queue_failed', result_code))

            # å‘é€gpt_loadé˜Ÿåˆ—
            if checkpoint.wait_send_gpt_load and self.gpt_load_enabled:
                gpt_load_keys = list(checkpoint.wait_send_gpt_load)
                logger.info(t('processing_gpt_load_queue', len(gpt_load_keys)))

                result_code = self._send_gpt_load_worker(gpt_load_keys)

                if result_code == 'ok':
                    # æ¸…ç©ºé˜Ÿåˆ—
                    checkpoint.wait_send_gpt_load.clear()
                    logger.info(t('gpt_load_queue_cleared', len(gpt_load_keys)))
                else:
                    logger.error(t('gpt_load_queue_failed', result_code))

            # ä¿å­˜checkpoint
            file_manager.save_checkpoint(checkpoint)
        except Exception as e:
            stacktrace = traceback.format_exc()
            logger.error(t('batch_send_error', e) + f"\n{stacktrace}")
        finally:
            self.saving_checkpoint = False  # Release the lock

    def shutdown(self) -> None:
        """å…³é—­çº¿ç¨‹æ± å’Œå®šæ—¶å™¨"""
        self.shutdown_flag = True

        if self.batch_timer:
            self.batch_timer.cancel()

        self.executor.shutdown(wait=True)
        logger.info(t('syncutils_shutdown_complete'))


# åˆ›å»ºå…¨å±€å®ä¾‹
sync_utils = SyncUtils()

